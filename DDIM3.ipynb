{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92ad01f2-7512-45c3-b104-95219a9431d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "#############################################\n",
    "# Utility functions for FFT and iFFT\n",
    "#############################################\n",
    "def complex_fft2(x):\n",
    "    # x: [B, 2, H, W] with (real, imag) in channels.\n",
    "    real = x[:, 0, :, :]\n",
    "    imag = x[:, 1, :, :]\n",
    "    x_complex = torch.complex(real, imag)\n",
    "    X_complex = torch.fft.fft2(x_complex, norm='ortho')\n",
    "    return torch.stack([X_complex.real, X_complex.imag], dim=1)\n",
    "\n",
    "def complex_ifft2(X):\n",
    "    X_complex = torch.complex(X[:, 0, :, :], X[:, 1, :, :])\n",
    "    x_complex = torch.fft.ifft2(X_complex, norm='ortho')\n",
    "    return torch.stack([x_complex.real, x_complex.imag], dim=1)\n",
    "\n",
    "#############################################\n",
    "# Model definitions (DownBlock, UpBlock, UNet, DDIM)\n",
    "#############################################\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    def forward(self, x):\n",
    "        x_conv = self.conv(x)\n",
    "        x_down = self.pool(x_conv)\n",
    "        return x_conv, x_down\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * 2, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        diffH = skip.size(2) - x.size(2)\n",
    "        diffW = skip.size(3) - x.size(3)\n",
    "        if diffH > 0 or diffW > 0:\n",
    "            x = F.pad(x, (0, diffW, 0, diffH), mode='constant', value=0)\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=2, out_channels=2, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        prev_channels = in_channels\n",
    "        for f in features:\n",
    "            self.down_blocks.append(DownBlock(prev_channels, f))\n",
    "            prev_channels = f\n",
    "        reversed_feats = list(reversed(features))\n",
    "        for i in range(len(reversed_feats) - 1):\n",
    "            self.up_blocks.append(UpBlock(reversed_feats[i], reversed_feats[i+1]))\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "    def forward(self, x, t=None, cond=None):\n",
    "        if cond is not None:\n",
    "            x = torch.cat([x, cond], dim=1)\n",
    "        skip_connections = []\n",
    "        out = x\n",
    "        for down in self.down_blocks:\n",
    "            skip, out = down(out)\n",
    "            skip_connections.append(skip)\n",
    "        bottom = skip_connections[-1]\n",
    "        skip_connections = skip_connections[:-1]\n",
    "        up_out = bottom\n",
    "        for i, up in enumerate(self.up_blocks):\n",
    "            skip_i = skip_connections[-(i+1)]\n",
    "            up_out = up(up_out, skip_i)\n",
    "        return self.final_conv(up_out)\n",
    "\n",
    "class DDIM(nn.Module):\n",
    "    def __init__(self, unet, timesteps=1000, beta_start=1e-4, beta_end=0.02, schedule=\"linear\", device='cpu'):\n",
    "        super().__init__()\n",
    "        self.unet = unet.to(device)\n",
    "        self.timesteps = timesteps\n",
    "        self.device = device\n",
    "        if schedule == \"linear\":\n",
    "            self.betas = torch.linspace(beta_start, beta_end, timesteps, device=device)\n",
    "        elif schedule == \"cosine\":\n",
    "            self.betas = self._cosine_beta_schedule(timesteps).to(device)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown schedule: {schedule}\")\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = torch.cat([torch.tensor([1.0], device=device), self.alphas_cumprod[:-1]])\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.posterior_variance = self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        \n",
    "    def _cosine_beta_schedule(self, timesteps, warmup_frac=0.02):\n",
    "        warmup_steps = int(timesteps * warmup_frac)\n",
    "        steps = torch.arange(timesteps + 1, dtype=torch.float64, device=self.device)\n",
    "        s = 0.008\n",
    "        alphas_cumprod_full = torch.cos(((steps / timesteps) + s) / (1 + s) * (torch.pi / 2))**2\n",
    "        alphas_cumprod_full = alphas_cumprod_full / alphas_cumprod_full[0]\n",
    "        if warmup_steps > 0:\n",
    "            alpha_end_warmup = alphas_cumprod_full[warmup_steps]\n",
    "            warmup_range = torch.linspace(1.0, alpha_end_warmup, warmup_steps + 1, device=self.device)\n",
    "            alphas_cumprod_full[:warmup_steps+1] = warmup_range\n",
    "        betas = 1 - (alphas_cumprod_full[1:] / alphas_cumprod_full[:-1])\n",
    "        return betas.float()\n",
    "        \n",
    "    def forward_diffusion(self, x_start, t):\n",
    "        noise = torch.randn_like(x_start)\n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t][:, None, None, None]\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t][:, None, None, None]\n",
    "        x_t = sqrt_alpha * x_start + sqrt_one_minus_alpha * noise\n",
    "        return x_t, noise\n",
    "        \n",
    "    def forward(self, x, t, cond=None):\n",
    "        return self.unet(x, t, cond=cond)\n",
    "        \n",
    "    def enforce_data_consistency_multicoil(self, x_pred, x_measured_k, mask):\n",
    "        \"\"\"\n",
    "        Enforces data consistency for multi-coil data.\n",
    "        x_pred:       [B, kx, ky, 2, coil] (image domain)\n",
    "        x_measured_k: [B, kx, ky, 2, coil] (measured k-space)\n",
    "        mask:         [B, kx, ky, 1, coil] (or with 2 in 4th dim)\n",
    "        \"\"\"\n",
    "        B, KX, KY, two, n_coils = x_pred.shape\n",
    "        if mask.shape[3] == 1:\n",
    "            mask = mask.repeat(1, 1, 1, 2, 1)\n",
    "        updated_coils = []\n",
    "        for c in range(n_coils):\n",
    "            real_pred = x_pred[:, :, :, 0, c]\n",
    "            imag_pred = x_pred[:, :, :, 1, c]\n",
    "            pred_complex = torch.complex(real_pred, imag_pred)\n",
    "            X_coil_pred = torch.fft.fft2(pred_complex, dim=(1,2), norm='ortho')\n",
    "            real_meas = x_measured_k[:, :, :, 0, c]\n",
    "            imag_meas = x_measured_k[:, :, :, 1, c]\n",
    "            X_coil_meas = torch.complex(real_meas, imag_meas)\n",
    "            coil_mask = mask[:, :, :, :, c]\n",
    "            if coil_mask.shape[3] == 2:\n",
    "                coil_mask = coil_mask[:, :, :, 0]\n",
    "            coil_mask = coil_mask.float().clamp(0, 1)\n",
    "            X_coil_updated = X_coil_pred * (1 - coil_mask) + X_coil_meas * coil_mask\n",
    "            x_coil_updated = torch.fft.ifft2(X_coil_updated, dim=(1,2), norm='ortho')\n",
    "            updated_coils.append(torch.stack([x_coil_updated.real, x_coil_updated.imag], dim=3))\n",
    "        x_pred_dc = torch.stack(updated_coils, dim=4)\n",
    "        return x_pred_dc\n",
    "        \n",
    "    def reverse_diffusion(self, x_t, t, x_measured_k=None, mask=None, eta=0):\n",
    "        cond = None\n",
    "        if (x_measured_k is not None) and (mask is not None):\n",
    "            cond = None\n",
    "        pred_noise = self.unet(x_t, t, cond=cond)\n",
    "        alpha = self.alphas[t][:, None, None, None]\n",
    "        alpha_cumprod = self.alphas_cumprod[t][:, None, None, None]\n",
    "        alpha_cumprod_prev = self.alphas_cumprod_prev[t][:, None, None, None]\n",
    "        eps = 1e-8\n",
    "        pred_x0 = (x_t - torch.sqrt(1 - alpha_cumprod) * pred_noise) / (torch.sqrt(alpha_cumprod) + eps)\n",
    "        pred_x0 = torch.clamp(pred_x0, -1, 1)\n",
    "        pred_mean = torch.sqrt(alpha_cumprod_prev) * pred_x0 + torch.sqrt(1 - alpha_cumprod_prev) * pred_noise\n",
    "        if eta > 0:\n",
    "            noise = torch.randn_like(x_t)\n",
    "            sigma = eta * torch.sqrt(self.posterior_variance[t])[:, None, None, None]\n",
    "            pred_mean = pred_mean + sigma * noise\n",
    "        if (x_measured_k is not None) and (mask is not None):\n",
    "            pred_mean = self.enforce_data_consistency_multicoil(\n",
    "                x_pred=pred_mean,\n",
    "                x_measured_k=x_measured_k,\n",
    "                mask=mask\n",
    "            )\n",
    "        return pred_mean\n",
    "        \n",
    "    def sample(self, shape, x_measured_k=None, mask=None, eta=0):\n",
    "        x_t = torch.randn(shape, device=self.device)\n",
    "        for timestep in reversed(range(self.timesteps)):\n",
    "            t_tensor = torch.full((shape[0],), timestep, device=self.device, dtype=torch.long)\n",
    "            x_t = self.reverse_diffusion(x_t, t_tensor, x_measured_k=x_measured_k, mask=mask, eta=eta)\n",
    "        return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f32ac2c3-a143-41ac-a8c5-3d3cb9eaa37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, eps=1e-9):\n",
    "        self.folder = folder\n",
    "        self.filenames = sorted([f for f in os.listdir(folder) if f.endswith('.pt')])\n",
    "        self.transform = transform\n",
    "        self.eps = eps\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    def __getitem__(self, idx):\n",
    "        filepath = os.path.join(self.folder, self.filenames[idx])\n",
    "        sample = torch.load(filepath)\n",
    "        if sample.ndim != 4:\n",
    "            raise ValueError(f\"Expected sample.ndim=4 ([kx, ky, 2, coil]), got {sample.ndim}.\")\n",
    "        if sample.shape[2] != 2:\n",
    "            raise ValueError(f\"Expected sample.shape[2]==2, got {sample.shape[2]}.\")\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        max_val = sample.abs().max()\n",
    "        if max_val > self.eps:\n",
    "            sample = sample / max_val\n",
    "        return sample\n",
    "\n",
    "def pad_collate_fn(batch, global_max_channels):\n",
    "    new_batch = []\n",
    "    for sample in batch:\n",
    "        if sample.shape[2] != 2:\n",
    "            if sample.shape[2] > 2:\n",
    "                sample = sample[:, :, :2, :]\n",
    "            else:\n",
    "                pad_channels = 2 - sample.shape[2]\n",
    "                sample = F.pad(sample, (0, pad_channels))\n",
    "        new_batch.append(sample)\n",
    "    batch = new_batch\n",
    "    max_kx = max(s.shape[0] for s in batch)\n",
    "    max_ky = max(s.shape[1] for s in batch)\n",
    "    padded_batch = []\n",
    "    for sample in batch:\n",
    "        kx, ky, two, coil = sample.shape\n",
    "        if coil < global_max_channels:\n",
    "            pad_coil = global_max_channels - coil\n",
    "            sample = F.pad(sample, (0, pad_coil))\n",
    "        elif coil > global_max_channels:\n",
    "            sample = sample[..., :global_max_channels]\n",
    "        padded = torch.zeros(max_kx, max_ky, 2, global_max_channels, dtype=sample.dtype)\n",
    "        padded[:kx, :ky, :, :] = sample\n",
    "        padded_batch.append(padded)\n",
    "    return torch.stack(padded_batch, dim=0)\n",
    "\n",
    "def compute_global_max_channels(data_folder):\n",
    "    max_coil = 0\n",
    "    for fname in os.listdir(data_folder):\n",
    "        if not fname.endswith('.pt'):\n",
    "            continue\n",
    "        path = os.path.join(data_folder, fname)\n",
    "        data = torch.load(path)\n",
    "        if data.ndim != 4:\n",
    "            raise ValueError(f\"File {fname}: expected 4D [kx, ky, 2, coil], got {data.ndim}.\")\n",
    "        if data.shape[2] != 2:\n",
    "            raise ValueError(f\"File {fname}: expected data.shape[2]==2, got {data.shape[2]}.\")\n",
    "        coil_dim = data.shape[3]\n",
    "        if coil_dim > max_coil:\n",
    "            max_coil = coil_dim\n",
    "    return max_coil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a3ba078-6640-4ded-b312-d04415170ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffusion_model_with_dc(\n",
    "    data_folder,\n",
    "    num_epochs=10,\n",
    "    lr=1e-4,\n",
    "    timesteps=1000,\n",
    "    batch_size=2,\n",
    "    schedule=\"linear\",\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02,\n",
    "    checkpoint_dir=None,\n",
    "    device=None,\n",
    "    final_model_path=None,\n",
    "    loss_function=\"mse\",\n",
    "    alpha=0.5,\n",
    "    weight_decay=1e-5\n",
    "):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    dataset = MRIDataset(data_folder)\n",
    "    total_len = len(dataset)\n",
    "    train_size = int(0.75 * total_len)\n",
    "    val_size = int(0.15 * total_len)\n",
    "    leftover_size = total_len - (train_size + val_size)\n",
    "    train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, leftover_size])\n",
    "    global_max_channels = 76  # or compute_global_max_channels(data_folder)\n",
    "    \n",
    "    print(\"Global_max_channels computed:\", global_max_channels)\n",
    "    collate_fn = lambda batch: pad_collate_fn(batch, global_max_channels)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    unet = UNet(\n",
    "        in_channels=2 * global_max_channels,\n",
    "        out_channels=2 * global_max_channels,\n",
    "        features=[64, 128, 256],\n",
    "    ).to(device)\n",
    "    ddim = DDIM(\n",
    "        unet=unet,\n",
    "        timesteps=timesteps,\n",
    "        beta_start=beta_start,\n",
    "        beta_end=beta_end,\n",
    "        schedule=schedule,\n",
    "        device=device\n",
    "    )\n",
    "    mse_criterion = nn.MSELoss()\n",
    "    l1_criterion = nn.L1Loss()\n",
    "    \n",
    "    def combined_loss(pred, target, alpha=alpha):\n",
    "        return alpha * l1_criterion(pred, target) + (1 - alpha) * mse_criterion(pred, target)\n",
    "    if loss_function.lower() == \"mse\":\n",
    "        criterion = mse_criterion\n",
    "    elif loss_function.lower() == \"l1\":\n",
    "        criterion = l1_criterion\n",
    "    elif loss_function.lower() == \"combined\":\n",
    "        criterion = lambda p, t: combined_loss(p, t, alpha=alpha)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss_function: {loss_function}.\")\n",
    "        \n",
    "    optimizer = optim.Adam(ddim.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    def estimate_x0_from_xt(x_t, pred_noise, t_indices):\n",
    "        alpha_cum = ddim.alphas_cumprod[t_indices].view(-1, 1, 1, 1)\n",
    "        sqrt_alpha_cum = torch.sqrt(alpha_cum.clamp(min=1e-8))\n",
    "        sqrt_one_minus_alpha = torch.sqrt(1.0 - alpha_cum)\n",
    "        return (x_t - sqrt_one_minus_alpha * pred_noise) / (sqrt_alpha_cum + 1e-8)\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        ddim.train()\n",
    "        train_loss_sum = 0.0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}/{num_epochs}\")):\n",
    "            B, KX, KY, two, coil = batch.shape\n",
    "            batch_4d = batch.permute(0, 3, 4, 1, 2).reshape(B, 2 * coil, KX, KY).to(device)\n",
    "            gt_image_4d = batch_4d\n",
    "            B_, c_, H, W_ = gt_image_4d.shape\n",
    "            coil_ = c_ // 2\n",
    "            gt_image_5d = gt_image_4d.reshape(B_, 2, coil_, H, W_).permute(0, 3, 4, 1, 2)\n",
    "            x_measured_k_5d = gt_image_5d.clone()\n",
    "            \n",
    "            for c_idx in range(coil_):\n",
    "                real_part = x_measured_k_5d[..., 0, c_idx]\n",
    "                imag_part = x_measured_k_5d[..., 1, c_idx]\n",
    "                coil_complex = torch.complex(real_part, imag_part)\n",
    "                X_coil = torch.fft.fft2(coil_complex, dim=(1, 2), norm='ortho')\n",
    "                x_measured_k_5d[..., 0, c_idx] = X_coil.real\n",
    "                x_measured_k_5d[..., 1, c_idx] = X_coil.imag\n",
    "            # For validation, use a fixed random seed so that the mask is more stable.\n",
    "            torch.manual_seed(42)\n",
    "            mask_2d = (torch.rand(H, W_, device=device) < 0.3)\n",
    "            torch.manual_seed(torch.initial_seed())  # reset to random\n",
    "            mask_5d = mask_2d[None, ..., None, None].repeat(B_, 1, 1, 1, coil_)\n",
    "            mask_5d = mask_5d.repeat(1, 1, 1, 2, 1)\n",
    "            x_measured_k_5d = x_measured_k_5d * mask_5d\n",
    "            t = torch.randint(0, timesteps, (B,), device=device).long()\n",
    "            with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                x_t, true_noise = ddim.forward_diffusion(gt_image_4d, t)\n",
    "                pred_noise_4d = ddim(x_t, t, cond=None)\n",
    "                pred_x0_4d = estimate_x0_from_xt(x_t, pred_noise_4d, t)\n",
    "                pred_x0_5d = pred_x0_4d.reshape(B, 2, coil, KX, KY).permute(0, 3, 4, 1, 2)\n",
    "                pred_x0_dc_5d = ddim.enforce_data_consistency_multicoil(\n",
    "                    x_pred=pred_x0_5d,\n",
    "                    x_measured_k=x_measured_k_5d,\n",
    "                    mask=mask_5d\n",
    "                )\n",
    "                pred_x0_dc_4d = pred_x0_dc_5d.permute(0, 3, 4, 1, 2).reshape(B, 2 * coil, KX, KY)\n",
    "                loss_noise = criterion(pred_noise_4d, true_noise)\n",
    "                loss_image = criterion(pred_x0_dc_4d, gt_image_4d)\n",
    "                loss = 0.5 * loss_noise + 0.5 * loss_image\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"[DEBUG] NaN loss at batch_idx={batch_idx}\")\n",
    "                break\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(ddim.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss_sum += loss.item()\n",
    "        avg_train_loss = train_loss_sum / len(train_loader)\n",
    "        ddim.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(tqdm(val_loader, desc=f\"[Val]   Epoch {epoch+1}/{num_epochs}\")):\n",
    "                B, KX, KY, two, coil = batch.shape\n",
    "                batch_4d = batch.permute(0, 3, 4, 1, 2).reshape(B, 2 * coil, KX, KY).to(device)\n",
    "                gt_image_4d = batch_4d\n",
    "                B_, c_, H, W_ = gt_image_4d.shape\n",
    "                coil_ = c_ // 2\n",
    "                gt_image_5d = gt_image_4d.reshape(B_, 2, coil_, H, W_).permute(0, 3, 4, 1, 2)\n",
    "                x_measured_k_5d = gt_image_5d.clone()\n",
    "                for c_idx in range(coil_):\n",
    "                    real_part = x_measured_k_5d[..., 0, c_idx]\n",
    "                    imag_part = x_measured_k_5d[..., 1, c_idx]\n",
    "                    coil_complex = torch.complex(real_part, imag_part)\n",
    "                    X_coil = torch.fft.fft2(coil_complex, dim=(1, 2), norm='ortho')\n",
    "                    x_measured_k_5d[..., 0, c_idx] = X_coil.real\n",
    "                    x_measured_k_5d[..., 1, c_idx] = X_coil.imag\n",
    "                torch.manual_seed(42)\n",
    "                mask_2d = (torch.rand(H, W_, device=device) < 0.3)\n",
    "                torch.manual_seed(torch.initial_seed())\n",
    "                mask_5d = mask_2d[None, ..., None, None].repeat(B_, 1, 1, 1, coil_)\n",
    "                mask_5d = mask_5d.repeat(1, 1, 1, 2, 1)\n",
    "                x_measured_k_5d = x_measured_k_5d * mask_5d\n",
    "                t = torch.randint(0, timesteps, (B,), device=device).long()\n",
    "                \n",
    "                with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    x_t, true_noise = ddim.forward_diffusion(gt_image_4d, t)\n",
    "                    pred_noise_4d = ddim(x_t, t, cond=None)\n",
    "                    pred_x0_4d = estimate_x0_from_xt(x_t, pred_noise_4d, t)\n",
    "                    pred_x0_5d = pred_x0_4d.reshape(B, 2, coil, KX, KY).permute(0, 3, 4, 1, 2)\n",
    "                    pred_x0_dc_5d = ddim.enforce_data_consistency_multicoil(\n",
    "                        x_pred=pred_x0_5d,\n",
    "                        x_measured_k=x_measured_k_5d,\n",
    "                        mask=mask_5d\n",
    "                    )\n",
    "                    pred_x0_dc_4d = pred_x0_dc_5d.permute(0, 3, 4, 1, 2).reshape(B, 2 * coil, KX, KY)\n",
    "                    loss_noise = criterion(pred_noise_4d, true_noise)\n",
    "                    loss_image = criterion(pred_x0_dc_4d, gt_image_4d)\n",
    "                    val_loss = 0.5 * loss_noise + 0.5 * loss_image\n",
    "                if torch.isnan(val_loss):\n",
    "                    print(f\"[DEBUG] NaN val_loss at batch_idx={batch_idx}\")\n",
    "                    break\n",
    "                val_loss_sum += val_loss.item()\n",
    "        avg_val_loss = val_loss_sum / len(val_loader)\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "        if checkpoint_dir is not None:\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"ddim_epoch_{epoch+1}.pt\")\n",
    "            torch.save(ddim.state_dict(), checkpoint_path)\n",
    "            print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "        if torch.isnan(torch.tensor(avg_train_loss)):\n",
    "            print(\"[DEBUG] Found NaN in training. Exiting epoch early.\")\n",
    "            break\n",
    "    if final_model_path is not None:\n",
    "        torch.save(ddim.state_dict(), final_model_path)\n",
    "        print(f\"Final model saved at: {final_model_path}\")\n",
    "    print(\"Training complete!\")\n",
    "    return ddim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585cadc8-48f0-47fb-8187-ef98977d1e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Global_max_channels computed: 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1/20: 100%|██████████| 1146/1146 [1:03:37<00:00,  3.33s/it]\n",
      "[Val]   Epoch 1/20: 100%|██████████| 229/229 [11:06<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Train Loss: 1.262975 | Val Loss: 1.261183\n",
      "Checkpoint saved: checkpoints/ddim_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 2/20: 100%|██████████| 1146/1146 [1:03:27<00:00,  3.32s/it]\n",
      "[Val]   Epoch 2/20: 100%|██████████| 229/229 [10:59<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Train Loss: 1.261122 | Val Loss: 1.261182\n",
      "Checkpoint saved: checkpoints/ddim_epoch_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 3/20: 100%|██████████| 1146/1146 [1:05:25<00:00,  3.43s/it]\n",
      "[Val]   Epoch 3/20: 100%|██████████| 229/229 [11:18<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Train Loss: 1.261122 | Val Loss: 1.261182\n",
      "Checkpoint saved: checkpoints/ddim_epoch_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 4/20: 100%|██████████| 1146/1146 [1:02:54<00:00,  3.29s/it]\n",
      "[Val]   Epoch 4/20: 100%|██████████| 229/229 [10:52<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Train Loss: 1.261122 | Val Loss: 1.261182\n",
      "Checkpoint saved: checkpoints/ddim_epoch_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/20: 100%|██████████| 1146/1146 [1:04:23<00:00,  3.37s/it]\n",
      "[Val]   Epoch 5/20: 100%|██████████| 229/229 [08:29<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Train Loss: 1.261122 | Val Loss: 1.261182\n",
      "Checkpoint saved: checkpoints/ddim_epoch_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 6/20: 100%|██████████| 1146/1146 [1:04:30<00:00,  3.38s/it]\n",
      "[Val]   Epoch 6/20: 100%|██████████| 229/229 [11:03<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Train Loss: 1.261122 | Val Loss: 1.261182\n",
      "Checkpoint saved: checkpoints/ddim_epoch_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 11/20: 100%|██████████| 1146/1146 [1:08:50<00:00,  3.60s/it]\n",
      "[Val]   Epoch 11/20: 100%|██████████| 229/229 [11:29<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Train Loss: 1.261123 | Val Loss: 1.261184\n",
      "Checkpoint saved: checkpoints/ddim_epoch_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 12/20: 100%|██████████| 1146/1146 [1:01:48<00:00,  3.24s/it]\n",
      "[Val]   Epoch 12/20: 100%|██████████| 229/229 [08:56<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Train Loss: 1.261124 | Val Loss: 1.261184\n",
      "Checkpoint saved: checkpoints/ddim_epoch_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 13/20: 100%|██████████| 1146/1146 [52:24<00:00,  2.74s/it]\n",
      "[Val]   Epoch 13/20: 100%|██████████| 229/229 [11:37<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Train Loss: 1.261125 | Val Loss: 1.261186\n",
      "Checkpoint saved: checkpoints/ddim_epoch_13.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 14/20: 100%|██████████| 1146/1146 [1:05:14<00:00,  3.42s/it]\n",
      "[Val]   Epoch 14/20: 100%|██████████| 229/229 [11:53<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] Train Loss: 1.261126 | Val Loss: 1.261187\n",
      "Checkpoint saved: checkpoints/ddim_epoch_14.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 15/20: 100%|██████████| 1146/1146 [1:06:34<00:00,  3.49s/it]\n",
      "[Val]   Epoch 15/20: 100%|██████████| 229/229 [11:50<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] Train Loss: 1.261127 | Val Loss: 1.261188\n",
      "Checkpoint saved: checkpoints/ddim_epoch_15.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 16/20: 100%|██████████| 1146/1146 [1:07:29<00:00,  3.53s/it]\n",
      "[Val]   Epoch 16/20: 100%|██████████| 229/229 [10:06<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Train Loss: 1.261128 | Val Loss: 1.261189\n",
      "Checkpoint saved: checkpoints/ddim_epoch_16.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 17/20: 100%|██████████| 1146/1146 [1:04:27<00:00,  3.37s/it]\n",
      "[Val]   Epoch 17/20: 100%|██████████| 229/229 [11:30<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] Train Loss: 1.261129 | Val Loss: 1.261189\n",
      "Checkpoint saved: checkpoints/ddim_epoch_17.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 18/20: 100%|██████████| 1146/1146 [1:05:44<00:00,  3.44s/it]\n",
      "[Val]   Epoch 18/20: 100%|██████████| 229/229 [11:08<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] Train Loss: 1.261129 | Val Loss: 1.261189\n",
      "Checkpoint saved: checkpoints/ddim_epoch_18.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 19/20:   1%|          | 8/1146 [00:28<1:10:21,  3.71s/it]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Path to your folder containing the .pt files\n",
    "    data_folder = \"mri-2\"\n",
    "    checkpoint_directory = \"checkpoints\"\n",
    "    final_model_path = \"models/new4_1_model.pt\"\n",
    "\n",
    "    # Train the model with the desired hyperparameters\n",
    "    trained_ddim_model = train_diffusion_model_with_dc(\n",
    "        data_folder,\n",
    "        num_epochs=20,\n",
    "        lr=1e-4,\n",
    "        timesteps=1000,\n",
    "        batch_size=4,\n",
    "        schedule=\"cosine\",\n",
    "        beta_start=1e-4,\n",
    "        beta_end=0.02,\n",
    "        checkpoint_dir=checkpoint_directory,\n",
    "        device=None, \n",
    "        final_model_path=final_model_path,\n",
    "        loss_function=\"combined\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bdd8381-b19f-4a32-b208-7424608949c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Checking Dataset Sample Shapes ===\n",
      "torch.Size([512, 208, 2, 15])\n",
      "torch.Size([512, 208, 2, 15])\n",
      "torch.Size([512, 208, 2, 15])\n",
      "torch.Size([512, 208, 2, 15])\n",
      "torch.Size([512, 208, 2, 15])\n"
     ]
    }
   ],
   "source": [
    "dataset = MRIDataset(\"mri-2\")\n",
    "\n",
    "print(\"=== Checking Dataset Sample Shapes ===\")\n",
    "for idx in range(min(5, len(dataset))):  # just print first 5\n",
    "    sample = dataset[idx]\n",
    "    print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ff0e912-aa7e-4658-93c1-2195dc016b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample shape: torch.Size([384, 144, 2, 34])\n",
      "Reshaped sample shape: torch.Size([68, 384, 144])\n",
      "After padding, sample shape: torch.Size([76, 384, 144])\n",
      "torch.Size([38, 384, 144])\n",
      "torch.Size([1, 384, 144])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.fft\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Load your trained model ---\n",
    "# Replace these hyperparameters with the ones you used during training.\n",
    "global_max_channels = 76         # for example, if you have 15 coils (15x2=30)\n",
    "features = [64, 128, 256]        # example feature sizes\n",
    "timesteps = 1000                 # diffusion timesteps, etc.\n",
    "schedule = \"cosine\"              # or \"cosine\"\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "unet = UNet(\n",
    "    in_channels=global_max_channels,    # input channels should match the global maximum channels\n",
    "    out_channels=global_max_channels,   # model outputs k-space data with the same channel count\n",
    "    features=features,\n",
    ")\n",
    "ddim = DDIM(\n",
    "    unet=unet,\n",
    "    timesteps=timesteps,\n",
    "    beta_start=beta_start,\n",
    "    beta_end=beta_end,\n",
    "    schedule=schedule,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load the saved state dictionary\n",
    "state_dict = torch.load(\"models/new3_1_model.pt\", map_location=device)\n",
    "ddim.load_state_dict(state_dict)\n",
    "ddim.to(device)\n",
    "ddim.eval()\n",
    "\n",
    "# --- Load a sample input ---\n",
    "# The sample is expected to be a .pt file with shape [kx, ky, 2, coil]\n",
    "sample = torch.load(\"mri-2/us_0058_3T_slice0_phase34.pt\", map_location=device)\n",
    "print(\"Original sample shape:\", sample.shape) \n",
    "\n",
    "# Permute to get [2, coil, kx, ky]\n",
    "sample = sample.permute(2, 3, 0, 1)\n",
    "\n",
    "sample = sample.reshape(-1, sample.shape[-2], sample.shape[-1])\n",
    "\n",
    "print(\"Reshaped sample shape:\", sample.shape)\n",
    "\n",
    "# Pad the sample along the channel dimension if needed.\n",
    "expected_channels = 76  # global_max_channels used during training\n",
    "if sample.shape[0] < expected_channels:\n",
    "    pad_channels = expected_channels - sample.shape[0]\n",
    "    sample = F.pad(sample, (0, 0, 0, 0, 0, pad_channels))\n",
    "    print(\"After padding, sample shape:\", sample.shape)\n",
    "elif sample.shape[0] > expected_channels:\n",
    "    # Alternatively, crop if the sample has more channels than expected.\n",
    "    sample = sample[:expected_channels, :, :]\n",
    "    print(\"After cropping, sample shape:\", sample.shape)\n",
    "\n",
    "# Add a batch dimension: now [1, C, H, W]\n",
    "sample = sample.unsqueeze(0).to(device)\n",
    "\n",
    "# --- Run the model ---\n",
    "with torch.no_grad():\n",
    "    t = torch.randint(0, timesteps, (sample.size(0),), device=device).long()\n",
    "    kspace_output = ddim(sample, t=t, cond=None)\n",
    "    # Expected kspace_output shape: [B, global_max_channels, H, W]\n",
    "\n",
    "# --- Convert k-space output to image domain ---\n",
    "B, C, H, W = kspace_output.shape\n",
    "if C % 2 != 0:\n",
    "    raise ValueError(\"Expected an even number of channels (coil*2).\")\n",
    "num_coils = C // 2\n",
    "\n",
    "# Reshape to separate coil and complex dimensions: [B, num_coils, 2, H, W]\n",
    "kspace_reshaped = kspace_output.view(B, num_coils, 2, H, W)\n",
    "# Convert to a complex tensor: channel 0 is real, channel 1 is imaginary\n",
    "kspace_complex = torch.complex(kspace_reshaped[:, :, 0, :, :], kspace_reshaped[:, :, 1, :, :])  # [B, num_coils, H, W]\n",
    "# kspace_complex = torch.fft.fftshift(kspace_complex)\n",
    "\n",
    "print(kspace_complex[0, :,:,:].shape)\n",
    "\n",
    "# Apply a 2D inverse FFT to get the image domain data (using orthogonal normalization)\n",
    "image_coils = torch.fft.ifft2(kspace_complex, dim=(-2, -1),  norm='ortho') # kspace_complex dimension not compatible\n",
    "\n",
    "# Compute the magnitude of each coil's image\n",
    "coil_magnitudes = torch.abs(image_coils)\n",
    "# Combine coils using the Sum-Of-Squares (SOS) method\n",
    "reconstructed_image = torch.sqrt(torch.sum(coil_magnitudes ** 2, dim=1))  # [B, H, W]\n",
    "img_min = reconstructed_image.min()\n",
    "img_max = reconstructed_image.max()\n",
    "reconstructed_image = (reconstructed_image - img_min) / (img_max - img_min + 1e-8)\n",
    "# Visualize the first image in the batch\n",
    "img_to_show = reconstructed_image[0].cpu().numpy()\n",
    "print(reconstructed_image.shape)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img_to_show, cmap='gray')\n",
    "plt.title(\"Reconstructed Image (IFFT of k-space Output)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6013384e-ecdd-4dfa-a9f6-c94a78a9431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Detected number of coils (first sample): 208\n",
      "Sample shape: torch.Size([30, 512, 208])\n",
      "Dataset split: 4581 train, 916 val, 612 test.\n",
      "Using combined L1 + MSE loss (alpha=0.5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1/10: 100%|██████████| 1146/1146 [52:01<00:00,  2.72s/it]\n",
      "[Val]   Epoch 1/10: 100%|██████████| 229/229 [10:57<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] | Train Loss: 0.783463 | Val Loss: 0.724070\n",
      "Saved checkpoint: checkpoints/ddim_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 2/10: 100%|██████████| 1146/1146 [52:54<00:00,  2.77s/it]\n",
      "[Val]   Epoch 2/10: 100%|██████████| 229/229 [11:01<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] | Train Loss: 0.692463 | Val Loss: 0.667421\n",
      "Saved checkpoint: checkpoints/ddim_epoch_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 3/10: 100%|██████████| 1146/1146 [56:37<00:00,  2.96s/it] \n",
      "[Val]   Epoch 3/10: 100%|██████████| 229/229 [08:53<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] | Train Loss: 0.646747 | Val Loss: 0.626818\n",
      "Saved checkpoint: checkpoints/ddim_epoch_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 4/10: 100%|██████████| 1146/1146 [46:20<00:00,  2.43s/it]\n",
      "[Val]   Epoch 4/10: 100%|██████████| 229/229 [11:03<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] | Train Loss: 0.617564 | Val Loss: 0.604356\n",
      "Saved checkpoint: checkpoints/ddim_epoch_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5/10: 100%|██████████| 1146/1146 [53:36<00:00,  2.81s/it]\n",
      "[Val]   Epoch 5/10: 100%|██████████| 229/229 [11:08<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] | Train Loss: 0.598560 | Val Loss: 0.590405\n",
      "Saved checkpoint: checkpoints/ddim_epoch_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 6/10: 100%|██████████| 1146/1146 [38:42<00:00,  2.03s/it]\n",
      "[Val]   Epoch 6/10: 100%|██████████| 229/229 [07:10<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] | Train Loss: 0.580483 | Val Loss: 0.577973\n",
      "Saved checkpoint: checkpoints/ddim_epoch_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.       | 161/1146 [07:31<49:30,  3.02s/it] \n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "[Train] Epoch 10/10: 100%|██████████| 1146/1146 [46:26<00:00,  2.43s/it]\n",
      "[Val]   Epoch 10/10: 100%|██████████| 229/229 [11:11<00:00,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] | Train Loss: 0.554453 | Val Loss: 0.549411\n",
      "Saved checkpoint: checkpoints/ddim_epoch_10.pt\n",
      "Final model saved at: models/new3_1_model.pt\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Path to your folder containing the .pt slice/phase files\n",
    "    data_folder = \"mri-2\"\n",
    "\n",
    "    checkpoint_directory = \"checkpoints\"\n",
    "    final_model_path = \"models/new3_1_model.pt\"\n",
    "\n",
    "    # Train the model with desired hyperparameters\n",
    "    trained_ddim_model = train_diffusion_model(\n",
    "        data_folder,\n",
    "        num_epochs=10,\n",
    "        lr=1e-4,\n",
    "        timesteps=1000,\n",
    "        batch_size=4,\n",
    "        schedule=\"cosine\",\n",
    "        beta_start=1e-4,\n",
    "        beta_end=0.02,\n",
    "        checkpoint_dir=checkpoint_directory,\n",
    "        device = None, \n",
    "        final_model_path=final_model_path,\n",
    "        loss_function=\"combined\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b4a1229-7c25-40fb-b211-8afee25a2e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample shape: torch.Size([384, 144, 2, 34])\n",
      "Reshaped sample shape: torch.Size([68, 384, 144])\n",
      "After padding, sample shape: torch.Size([76, 384, 144])\n",
      "torch.Size([38, 384, 144])\n",
      "torch.Size([1, 384, 144])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.fft\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Load your trained model ---\n",
    "# Replace these hyperparameters with the ones you used during training.\n",
    "global_max_channels = 76         # for example, if you have 15 coils (15x2=30)\n",
    "features = [64, 128, 256]        # example feature sizes\n",
    "timesteps = 1000                 # diffusion timesteps, etc.\n",
    "schedule = \"cosine\"              # or \"cosine\"\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "unet = UNet(\n",
    "    in_channels=global_max_channels,    # input channels should match the global maximum channels\n",
    "    out_channels=global_max_channels,   # model outputs k-space data with the same channel count\n",
    "    features=features,\n",
    ")\n",
    "ddim = DDIM(\n",
    "    unet=unet,\n",
    "    timesteps=timesteps,\n",
    "    beta_start=beta_start,\n",
    "    beta_end=beta_end,\n",
    "    schedule=schedule,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load the saved state dictionary\n",
    "state_dict = torch.load(\"models/new3_1_model.pt\", map_location=device)\n",
    "ddim.load_state_dict(state_dict)\n",
    "ddim.to(device)\n",
    "ddim.eval()\n",
    "\n",
    "# --- Load a sample input ---\n",
    "# The sample is expected to be a .pt file with shape [kx, ky, 2, coil]\n",
    "sample = torch.load(\"mri-2/us_0058_3T_slice0_phase34.pt\", map_location=device)\n",
    "print(\"Original sample shape:\", sample.shape) \n",
    "\n",
    "# Permute to get [2, coil, kx, ky]\n",
    "sample = sample.permute(2, 3, 0, 1)\n",
    "\n",
    "sample = sample.reshape(-1, sample.shape[-2], sample.shape[-1])\n",
    "\n",
    "print(\"Reshaped sample shape:\", sample.shape)\n",
    "\n",
    "# Pad the sample along the channel dimension if needed.\n",
    "expected_channels = 76  # global_max_channels used during training\n",
    "if sample.shape[0] < expected_channels:\n",
    "    pad_channels = expected_channels - sample.shape[0]\n",
    "    sample = F.pad(sample, (0, 0, 0, 0, 0, pad_channels))\n",
    "    print(\"After padding, sample shape:\", sample.shape)\n",
    "elif sample.shape[0] > expected_channels:\n",
    "    # Alternatively, crop if the sample has more channels than expected.\n",
    "    sample = sample[:expected_channels, :, :]\n",
    "    print(\"After cropping, sample shape:\", sample.shape)\n",
    "\n",
    "# Add a batch dimension: now [1, C, H, W]\n",
    "sample = sample.unsqueeze(0).to(device)\n",
    "\n",
    "# --- Run the model ---\n",
    "with torch.no_grad():\n",
    "    t = torch.randint(0, timesteps, (sample.size(0),), device=device).long()\n",
    "    kspace_output = ddim(sample, t=t, cond=None)\n",
    "    # Expected kspace_output shape: [B, global_max_channels, H, W]\n",
    "\n",
    "# --- Convert k-space output to image domain ---\n",
    "B, C, H, W = kspace_output.shape\n",
    "if C % 2 != 0:\n",
    "    raise ValueError(\"Expected an even number of channels (coil*2).\")\n",
    "num_coils = C // 2\n",
    "\n",
    "# Reshape to separate coil and complex dimensions: [B, num_coils, 2, H, W]\n",
    "kspace_reshaped = kspace_output.view(B, num_coils, 2, H, W)\n",
    "# Convert to a complex tensor: channel 0 is real, channel 1 is imaginary\n",
    "kspace_complex = torch.complex(kspace_reshaped[:, :, 0, :, :], kspace_reshaped[:, :, 1, :, :])  # [B, num_coils, H, W]\n",
    "# kspace_complex = torch.fft.fftshift(kspace_complex)\n",
    "\n",
    "print(kspace_complex[0, :,:,:].shape)\n",
    "\n",
    "# Apply a 2D inverse FFT to get the image domain data (using orthogonal normalization)\n",
    "image_coils = torch.fft.ifft2(kspace_complex, dim=(-2, -1),  norm='ortho') # kspace_complex dimension not compatible\n",
    "\n",
    "# Compute the magnitude of each coil's image\n",
    "coil_magnitudes = torch.abs(image_coils)\n",
    "# Combine coils using the Sum-Of-Squares (SOS) method\n",
    "reconstructed_image = torch.sqrt(torch.sum(coil_magnitudes ** 2, dim=1))  # [B, H, W]\n",
    "img_min = reconstructed_image.min()\n",
    "img_max = reconstructed_image.max()\n",
    "reconstructed_image = (reconstructed_image - img_min) / (img_max - img_min + 1e-8)\n",
    "# Visualize the first image in the batch\n",
    "img_to_show = reconstructed_image[0].cpu().numpy()\n",
    "print(reconstructed_image.shape)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img_to_show, cmap='gray')\n",
    "plt.title(\"Reconstructed Image (IFFT of k-space Output)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2c3f7-193a-423c-95c9-17f4581755e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
